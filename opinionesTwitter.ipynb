{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerías a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos el csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensajesTwitter_sin =  pd.read_csv(\"datas/calentamientoClimatico_sin_preparar.csv\",delimiter=\";\", encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1</th>\n",
       "      <th>Column2</th>\n",
       "      <th>Column3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tweet</td>\n",
       "      <td>existence</td>\n",
       "      <td>existence.confidence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Global warming report urges governments to act...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fighting poverty and global warming in Africa ...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.8786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6086</th>\n",
       "      <td>@bloodless_coup \"The phrase 'global warming' s...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6087</th>\n",
       "      <td>Virginia to Investigate Global Warming Scienti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6088</th>\n",
       "      <td>Global warming you tube parody you will enjoy ...</td>\n",
       "      <td>No</td>\n",
       "      <td>0.6411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6089</th>\n",
       "      <td>One-Eyed Golfer: Don't dare tell me about glob...</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6090</th>\n",
       "      <td>man made global warming a hair brained theory ...</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6091 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Column1    Column2  \\\n",
       "0                                                 tweet  existence   \n",
       "1     Global warming report urges governments to act...        Yes   \n",
       "2     Fighting poverty and global warming in Africa ...        Yes   \n",
       "3     Carbon offsets: How a Vatican forest failed to...        Yes   \n",
       "4     Carbon offsets: How a Vatican forest failed to...        Yes   \n",
       "...                                                 ...        ...   \n",
       "6086  @bloodless_coup \"The phrase 'global warming' s...        Yes   \n",
       "6087  Virginia to Investigate Global Warming Scienti...        NaN   \n",
       "6088  Global warming you tube parody you will enjoy ...         No   \n",
       "6089  One-Eyed Golfer: Don't dare tell me about glob...         No   \n",
       "6090  man made global warming a hair brained theory ...         No   \n",
       "\n",
       "                   Column3  \n",
       "0     existence.confidence  \n",
       "1                        1  \n",
       "2                        1  \n",
       "3                   0.8786  \n",
       "4                        1  \n",
       "...                    ...  \n",
       "6086                     1  \n",
       "6087                     1  \n",
       "6088                0.6411  \n",
       "6089                     1  \n",
       "6090                     1  \n",
       "\n",
       "[6091 rows x 3 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensajesTwitter_sin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploración de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6091 entries, 0 to 6090\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Column1  6091 non-null   object\n",
      " 1   Column2  4226 non-null   object\n",
      " 2   Column3  6088 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 142.9+ KB\n"
     ]
    }
   ],
   "source": [
    "mensajesTwitter_sin.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miramos si hay nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column1       0\n",
       "Column2    1865\n",
       "Column3       3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensajesTwitter_sin.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1</th>\n",
       "      <th>Column2</th>\n",
       "      <th>Column3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6091</td>\n",
       "      <td>4226</td>\n",
       "      <td>6088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5542</td>\n",
       "      <td>3</td>\n",
       "      <td>1760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>No matter if you believe in global warming or ...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>20</td>\n",
       "      <td>3111</td>\n",
       "      <td>2391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Column1 Column2 Column3\n",
       "count                                                6091    4226    6088\n",
       "unique                                               5542       3    1760\n",
       "top     No matter if you believe in global warming or ...     Yes       1\n",
       "freq                                                   20    3111    2391"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensajesTwitter_sin.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6091, 3)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensajesTwitter_sin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'existence'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Carlota Martin-Anero\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Carlota Martin-Anero\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Carlota Martin-Anero\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'existence'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\CARLOT~1\\AppData\\Local\\Temp/ipykernel_27164/1160623697.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcountplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmensajesTwitter_sin\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'existence'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Carlota Martin-Anero\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3458\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Carlota Martin-Anero\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'existence'"
     ]
    }
   ],
   "source": [
    "sns.countplot(mensajesTwitter_sin['existence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambiamos el nombre de las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensajesTwitter_sin.columns = ['tweet', 'existence', 'existence.confidence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la primera fila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensajesTwitter_sin = mensajesTwitter_sin.drop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que está bien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>existence</th>\n",
       "      <th>existence.confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Global warming report urges governments to act...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fighting poverty and global warming in Africa ...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.8786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>URUGUAY: Tools Needed for Those Most Vulnerabl...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.8087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6086</th>\n",
       "      <td>@bloodless_coup \"The phrase 'global warming' s...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6087</th>\n",
       "      <td>Virginia to Investigate Global Warming Scienti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6088</th>\n",
       "      <td>Global warming you tube parody you will enjoy ...</td>\n",
       "      <td>No</td>\n",
       "      <td>0.6411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6089</th>\n",
       "      <td>One-Eyed Golfer: Don't dare tell me about glob...</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6090</th>\n",
       "      <td>man made global warming a hair brained theory ...</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6090 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet existence  \\\n",
       "1     Global warming report urges governments to act...       Yes   \n",
       "2     Fighting poverty and global warming in Africa ...       Yes   \n",
       "3     Carbon offsets: How a Vatican forest failed to...       Yes   \n",
       "4     Carbon offsets: How a Vatican forest failed to...       Yes   \n",
       "5     URUGUAY: Tools Needed for Those Most Vulnerabl...       Yes   \n",
       "...                                                 ...       ...   \n",
       "6086  @bloodless_coup \"The phrase 'global warming' s...       Yes   \n",
       "6087  Virginia to Investigate Global Warming Scienti...       NaN   \n",
       "6088  Global warming you tube parody you will enjoy ...        No   \n",
       "6089  One-Eyed Golfer: Don't dare tell me about glob...        No   \n",
       "6090  man made global warming a hair brained theory ...        No   \n",
       "\n",
       "     existence.confidence  \n",
       "1                       1  \n",
       "2                       1  \n",
       "3                  0.8786  \n",
       "4                       1  \n",
       "5                  0.8087  \n",
       "...                   ...  \n",
       "6086                    1  \n",
       "6087                    1  \n",
       "6088               0.6411  \n",
       "6089                    1  \n",
       "6090                    1  \n",
       "\n",
       "[6090 rows x 3 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensajesTwitter_sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>existence</th>\n",
       "      <th>existence.confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Global warming report urges governments to act...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fighting poverty and global warming in Africa ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>URUGUAY: Tools Needed for Those Most Vulnerabl...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6086</th>\n",
       "      <td>@bloodless_coup \"The phrase 'global warming' s...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6087</th>\n",
       "      <td>Virginia to Investigate Global Warming Scienti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6088</th>\n",
       "      <td>Global warming you tube parody you will enjoy ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6089</th>\n",
       "      <td>One-Eyed Golfer: Don't dare tell me about glob...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6090</th>\n",
       "      <td>man made global warming a hair brained theory ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6090 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  existence  \\\n",
       "1     Global warming report urges governments to act...        1.0   \n",
       "2     Fighting poverty and global warming in Africa ...        1.0   \n",
       "3     Carbon offsets: How a Vatican forest failed to...        1.0   \n",
       "4     Carbon offsets: How a Vatican forest failed to...        1.0   \n",
       "5     URUGUAY: Tools Needed for Those Most Vulnerabl...        1.0   \n",
       "...                                                 ...        ...   \n",
       "6086  @bloodless_coup \"The phrase 'global warming' s...        1.0   \n",
       "6087  Virginia to Investigate Global Warming Scienti...        NaN   \n",
       "6088  Global warming you tube parody you will enjoy ...        0.0   \n",
       "6089  One-Eyed Golfer: Don't dare tell me about glob...        0.0   \n",
       "6090  man made global warming a hair brained theory ...        0.0   \n",
       "\n",
       "     existence.confidence  \n",
       "1                       1  \n",
       "2                       1  \n",
       "3                  0.8786  \n",
       "4                       1  \n",
       "5                  0.8087  \n",
       "...                   ...  \n",
       "6086                    1  \n",
       "6087                    1  \n",
       "6088               0.6411  \n",
       "6089                    1  \n",
       "6090                    1  \n",
       "\n",
       "[6090 rows x 3 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensajesTwitter_sin['existence'] = mensajesTwitter_sin['existence'].replace({'Yes': 1, 'No': 0})\n",
    "mensajesTwitter_sin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos los nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>existence</th>\n",
       "      <th>existence.confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Global warming report urges governments to act...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fighting poverty and global warming in Africa ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>URUGUAY: Tools Needed for Those Most Vulnerabl...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6085</th>\n",
       "      <td>It's 83_Á and climbing in NYC. August weather...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6086</th>\n",
       "      <td>@bloodless_coup \"The phrase 'global warming' s...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6088</th>\n",
       "      <td>Global warming you tube parody you will enjoy ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6089</th>\n",
       "      <td>One-Eyed Golfer: Don't dare tell me about glob...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6090</th>\n",
       "      <td>man made global warming a hair brained theory ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4225 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  existence  \\\n",
       "1     Global warming report urges governments to act...        1.0   \n",
       "2     Fighting poverty and global warming in Africa ...        1.0   \n",
       "3     Carbon offsets: How a Vatican forest failed to...        1.0   \n",
       "4     Carbon offsets: How a Vatican forest failed to...        1.0   \n",
       "5     URUGUAY: Tools Needed for Those Most Vulnerabl...        1.0   \n",
       "...                                                 ...        ...   \n",
       "6085  It's 83_Á and climbing in NYC. August weather...        1.0   \n",
       "6086  @bloodless_coup \"The phrase 'global warming' s...        1.0   \n",
       "6088  Global warming you tube parody you will enjoy ...        0.0   \n",
       "6089  One-Eyed Golfer: Don't dare tell me about glob...        0.0   \n",
       "6090  man made global warming a hair brained theory ...        0.0   \n",
       "\n",
       "     existence.confidence  \n",
       "1                       1  \n",
       "2                       1  \n",
       "3                  0.8786  \n",
       "4                       1  \n",
       "5                  0.8087  \n",
       "...                   ...  \n",
       "6085                    1  \n",
       "6086                    1  \n",
       "6088               0.6411  \n",
       "6089                    1  \n",
       "6090                    1  \n",
       "\n",
       "[4225 rows x 3 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensajesTwitter_sin = mensajesTwitter_sin.dropna()\n",
    "mensajesTwitter_sin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que se han eliminado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet                   0\n",
       "existence               0\n",
       "existence.confidence    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensajesTwitter_sin.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4225, 3)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensajesTwitter_sin.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos limpiado nuestro dataset, por lo tanto vamos a empezar a hacer el proceso de normalización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este paso vamos a utilizar la librería re (regular expression).\n",
    "\n",
    "Vamos a utilizar una funcion llama da normalización, que nos va a permitir buscar un conjunto de carácteres específicos en una cadena de caracteres. Cuando los encuentre le aplicaremos una transformación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizacion(mensaje):\n",
    "    mensaje = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', mensaje) #la primer sentencia en '' la cambia por lo segunda ''\n",
    "    mensaje = re.sub('@[^\\s]+','USER', mensaje)\n",
    "    mensaje = mensaje.lower().replace(\"ё\", \"е\")\n",
    "    mensaje = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', mensaje)\n",
    "    mensaje = re.sub(' +',' ', mensaje)\n",
    "    return mensaje.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lo aplicamos a nuestro dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CARLOT~1\\AppData\\Local\\Temp/ipykernel_27164/296542153.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mensajesTwitter_sin ['tweet'] = mensajesTwitter_sin['tweet'].apply(normalizacion)\n"
     ]
    }
   ],
   "source": [
    "mensajesTwitter_sin ['tweet'] = mensajesTwitter_sin['tweet'].apply(normalizacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>existence</th>\n",
       "      <th>existence.confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>global warming report urges governments to act...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fighting poverty and global warming in africa ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>carbon offsets how a vatican forest failed to ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>carbon offsets how a vatican forest failed to ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>uruguay tools needed for those most vulnerable...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6085</th>\n",
       "      <td>it s 83 and climbing in nyc august weather for...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6086</th>\n",
       "      <td>user the phrase global warming should be aband...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6088</th>\n",
       "      <td>global warming you tube parody you will enjoy ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6089</th>\n",
       "      <td>one eyed golfer don t dare tell me about globa...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6090</th>\n",
       "      <td>man made global warming a hair brained theory ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4225 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  existence  \\\n",
       "1     global warming report urges governments to act...        1.0   \n",
       "2     fighting poverty and global warming in africa ...        1.0   \n",
       "3     carbon offsets how a vatican forest failed to ...        1.0   \n",
       "4     carbon offsets how a vatican forest failed to ...        1.0   \n",
       "5     uruguay tools needed for those most vulnerable...        1.0   \n",
       "...                                                 ...        ...   \n",
       "6085  it s 83 and climbing in nyc august weather for...        1.0   \n",
       "6086  user the phrase global warming should be aband...        1.0   \n",
       "6088  global warming you tube parody you will enjoy ...        0.0   \n",
       "6089  one eyed golfer don t dare tell me about globa...        0.0   \n",
       "6090  man made global warming a hair brained theory ...        0.0   \n",
       "\n",
       "     existence.confidence  \n",
       "1                       1  \n",
       "2                       1  \n",
       "3                  0.8786  \n",
       "4                       1  \n",
       "5                  0.8087  \n",
       "...                   ...  \n",
       "6085                    1  \n",
       "6086                    1  \n",
       "6088               0.6411  \n",
       "6089                    1  \n",
       "6090                    1  \n",
       "\n",
       "[4225 rows x 3 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensajesTwitter_sin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminación de las stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las stop words son las palabras de un idioma más utilizadas y que no aportan valor añadido al sentido de la frase. Por tanto las eliminaremos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a importas la librería nlkd, que nos ayudará a eliminar las stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero me descargo las stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Carlota Martin-\n",
      "[nltk_data]     Anero\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar una función lambda (funciones que solo pueden contener una expresión) que creara una cadena vacía de texto.\n",
    "\n",
    "Si al ir recorrindo las palabras, la variables stopWords no lo reconoce (en su diccionario), la añade a la cadena vacía; si la conoce, pasa a la siguiente palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                tweet  existence  \\\n",
      "1   global warming report urges governments act br...        1.0   \n",
      "2         fighting poverty global warming africa link        1.0   \n",
      "3   carbon offsets vatican forest failed reduce gl...        1.0   \n",
      "4   carbon offsets vatican forest failed reduce gl...        1.0   \n",
      "5   uruguay tools needed vulnerable climate change...        1.0   \n",
      "6   rt user rt user ocean saltiness shows global w...        1.0   \n",
      "7   global warming evidence around us message glob...        1.0   \n",
      "8   migratory birds new climate change strategy st...        1.0   \n",
      "9   southern africa competing limpopo water climat...        1.0   \n",
      "10  global warming impact wheat rice production in...        1.0   \n",
      "\n",
      "   existence.confidence  \n",
      "1                     1  \n",
      "2                     1  \n",
      "3                0.8786  \n",
      "4                     1  \n",
      "5                0.8087  \n",
      "6                     1  \n",
      "7                     1  \n",
      "8                     1  \n",
      "9                     1  \n",
      "10                    1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CARLOT~1\\AppData\\Local\\Temp/ipykernel_27164/2569005379.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mensajesTwitter_sin['tweet'] = mensajesTwitter_sin['tweet'].apply(lambda mensaje: ' '.join([palabra for palabra in mensaje.split() if palabra not in (stopWords)]))\n"
     ]
    }
   ],
   "source": [
    "mensajesTwitter_sin['tweet'] = mensajesTwitter_sin['tweet'].apply(lambda mensaje: ' '.join([palabra for palabra in mensaje.split() if palabra not in (stopWords)]))\n",
    "print(mensajesTwitter_sin.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                tweet  existence  \\\n",
      "1   global warm report urg govern act brussel belg...        1.0   \n",
      "2               fight poverti global warm africa link        1.0   \n",
      "3   carbon offset vatican forest fail reduc global...        1.0   \n",
      "4   carbon offset vatican forest fail reduc global...        1.0   \n",
      "5          uruguay tool need vulner climat chang link        1.0   \n",
      "6   rt user rt user ocean salti show global warm i...        1.0   \n",
      "7   global warm evid around us messag global warm ...        1.0   \n",
      "8   migratori bird new climat chang strategi stay ...        1.0   \n",
      "9   southern africa compet limpopo water climat ch...        1.0   \n",
      "10  global warm impact wheat rice product india lu...        1.0   \n",
      "\n",
      "   existence.confidence  \n",
      "1                     1  \n",
      "2                     1  \n",
      "3                0.8786  \n",
      "4                     1  \n",
      "5                0.8087  \n",
      "6                     1  \n",
      "7                     1  \n",
      "8                     1  \n",
      "9                     1  \n",
      "10                    1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CARLOT~1\\AppData\\Local\\Temp/ipykernel_27164/1707050472.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mensajesTwitter_sin['tweet'] = mensajesTwitter_sin['tweet'].apply(lambda mensaje: ' '.join([stemmer.stem(palabra) for palabra in mensaje.split(' ')]))\n"
     ]
    }
   ],
   "source": [
    "#Aplicación de stemming\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "mensajesTwitter_sin['tweet'] = mensajesTwitter_sin['tweet'].apply(lambda mensaje: ' '.join([stemmer.stem(palabra) for palabra in mensaje.split(' ')]))\n",
    "print(mensajesTwitter_sin.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La lematización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Carlota Martin-\n",
      "[nltk_data]     Anero\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Carlota Martin-\n",
      "[nltk_data]     Anero\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                tweet  existence  \\\n",
      "1   global warm report urg govern act brussel belg...        1.0   \n",
      "2               fight poverti global warm africa link        1.0   \n",
      "3   carbon offset vatican forest fail reduc global...        1.0   \n",
      "4   carbon offset vatican forest fail reduc global...        1.0   \n",
      "5          uruguay tool need vulner climat chang link        1.0   \n",
      "6   rt user rt user ocean salti show global warm i...        1.0   \n",
      "7   global warm evid around u messag global warm d...        1.0   \n",
      "8   migratori bird new climat chang strategi stay ...        1.0   \n",
      "9   southern africa compet limpopo water climat ch...        1.0   \n",
      "10  global warm impact wheat rice product india lu...        1.0   \n",
      "\n",
      "   existence.confidence  \n",
      "1                     1  \n",
      "2                     1  \n",
      "3                0.8786  \n",
      "4                     1  \n",
      "5                0.8087  \n",
      "6                     1  \n",
      "7                     1  \n",
      "8                     1  \n",
      "9                     1  \n",
      "10                    1  \n",
      "¡Fin de la preparación!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CARLOT~1\\AppData\\Local\\Temp/ipykernel_27164/1649731488.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mensajesTwitter_sin['tweet']= mensajesTwitter_sin['tweet'].apply(lambda mensaje: ' '.join([lemmatizer.lemmatize(palabra) for palabra in mensaje.split(' ')]))\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "mensajesTwitter_sin['tweet']= mensajesTwitter_sin['tweet'].apply(lambda mensaje: ' '.join([lemmatizer.lemmatize(palabra) for palabra in mensaje.split(' ')]))\n",
    "print(mensajesTwitter_sin.head(10))\n",
    "\n",
    "print(\"¡Fin de la preparación!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos los datos preparados y podemos empezar la predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fases de aprendizaje y predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerías a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos nuestro dataset, en variables dependientes e independietes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mensajesTwitter_sin['tweet'].values\n",
    "y = mensajesTwitter_sin['existence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eltapas de aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explicarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primero importamos las librerías a usar\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "etapas_aprendizaje = Pipeline([('frequencia', CountVectorizer()),('tfidf', TfidfTransformer()),('algoritmo', MultinomialNB())])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizamos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = etapas_aprendizaje.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8667    0.2675    0.4088       243\n",
      "         1.0     0.7688    0.9834    0.8630       602\n",
      "\n",
      "    accuracy                         0.7775       845\n",
      "   macro avg     0.8177    0.6254    0.6359       845\n",
      "weighted avg     0.7970    0.7775    0.7324       845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, modelo.predict(X_test), digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponemos a prueba nuestra predicción con una frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is Carlota and I like cats and big horses\n"
     ]
    }
   ],
   "source": [
    "frase = \"Hello my name is Carlota and I like cats and big horses\"\n",
    "print(frase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizamos la frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = normalizacion(frase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos las stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = ' '.join([palabra for palabra in frase.split() if palabra not in (stopWords)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos el stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase =  ' '.join([stemmer.stem(palabra) for palabra in frase.split(' ')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lematizamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello name carlota like cat big hors\n"
     ]
    }
   ],
   "source": [
    "frase = ' '.join([lemmatizer.lemmatize(palabra) for palabra in   frase.split(' ')])  \n",
    "#Comporbamos que lo hemos hecho correcto\n",
    "print (frase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n",
      ">> Vota a PACMAN...\n"
     ]
    }
   ],
   "source": [
    "prediccion = modelo.predict([frase])\n",
    "print(prediccion)\n",
    "if(prediccion[0]==0):\n",
    "    print(\">> No le gustan los gatos ni los caballos...\")\n",
    "else:\n",
    "    print(\">> Vota a PACMAN...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emocionada me encuentro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se trata del algortimo SVM (maquina de vectores de soporte) para clasificar el texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "etapas_aprendizaje = Pipeline([('frequencia', CountVectorizer()),\n",
    "                                ('tfidf', TfidfTransformer()),\n",
    "                                ('algoritmo', svm.SVC(kernel='linear', C=2))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos el aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = etapas_aprendizaje.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7538    0.6173    0.6787       243\n",
      "         1.0     0.8560    0.9186    0.8862       602\n",
      "\n",
      "    accuracy                         0.8320       845\n",
      "   macro avg     0.8049    0.7679    0.7825       845\n",
      "weighted avg     0.8266    0.8320    0.8266       845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, modelo.predict(X_test),  \n",
    "digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar la función GridSearchCV para hacer mejor el algoritmo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos librerías\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algoritmo__C': 1}\n"
     ]
    }
   ],
   "source": [
    "parameters = {'algoritmo__C':(1,2,4,5,6,7,8,9,10,11,12)}\n",
    "\n",
    "clf = GridSearchCV(etapas_aprendizaje, parameters,cv=2)\n",
    "clf.fit(X_train,y_train)\n",
    "print(clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7609    0.5761    0.6557       243\n",
      "         1.0     0.8442    0.9269    0.8836       602\n",
      "\n",
      "    accuracy                         0.8260       845\n",
      "   macro avg     0.8025    0.7515    0.7697       845\n",
      "weighted avg     0.8202    0.8260    0.8181       845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "etapas_aprendizaje = Pipeline([('frequencia', CountVectorizer()),\n",
    "                                ('tfidf', TfidfTransformer()),\n",
    "                                ('algoritmo', svm.SVC(kernel='linear', C=1))])\n",
    "\n",
    "modelo = etapas_aprendizaje.fit(X_train,y_train)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, modelo.predict(X_test), digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisión del 85%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4df6de8a2ad968c3d88ac569506327777d0b086099f01f429e5f01001212510a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
